services:
  digitalisation_toolkit-nginx:
    container_name: digitalisation_toolkit-nginx
    image: nginx:1.27.2-alpine
    ports:
      - "8080:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    networks:
      - shared-network
    restart: unless-stopped

  digitalisation_toolkit-frontend:
    container_name: digitalisation_toolkit-frontend
    restart: unless-stopped
    build:
      context: ./frontend
    env_file:
      - ./frontend/.env
    networks:
      - shared-network
    depends_on:
      - digitalisation_toolkit-nginx

  digitalisation_toolkit-backend:
    container_name: digitalisation_toolkit-backend
    build:
      context: ./backend
    env_file:
      - ./backend/.env
    networks:
      - shared-network
    restart: unless-stopped
    depends_on:
      - digitalisation_toolkit-nginx

  vllm_Qwen2.5-Coder-14B-Instruct:
    container_name: vllm_Qwen2.5-Coder-14B-Instruct
    image: vllm/vllm-openai:v0.6.6
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ./models/Qwen2.5-Coder-14B-Instruct:/models/Qwen2.5-Coder-14B-Instruct
    networks:
      - shared-network
    ipc: host
    environment:
      - HF_HUB_OFFLINE=1
    command: [
      "--model", "/models/Qwen2.5-Coder-14B-Instruct",
      "--enforce-eager",
      "--served-model-name", "Qwen2.5-Coder-14B-Instruct",
      "--trust-remote-code",
      "--guided-decoding-backend", "outlines",
      "--gpu-memory-utilization", "0.8"
    ]

  vllm_Gemma-SEA-LION-v3-9B-IT:
    container_name: vllm_Gemma-SEA-LION-v3-9B-IT
    image: vllm/vllm-openai:v0.6.6
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ./models/Gemma2-9B-Sealion-V3-Instruct:/models/Gemma-SEA-LION-v3-9B-IT
    networks:
      - shared-network
    ipc: host
    environment:
      - HF_HUB_OFFLINE=1
    command: [
      "--dtype", "half",
      "--max-model-len", "8192",
      "--enforce-eager",
      "--served-model-name", "Gemma-SEA-LION-v3-9B-IT",
      "--model", "/models/Gemma-SEA-LION-v3-9B-IT",
      "--trust-remote-code",
      "--guided-decoding-backend", "outlines",
      "--gpu-memory-utilization", "0.5"
    ]

networks:
  shared-network:
    name: shared-network
    driver: bridge

